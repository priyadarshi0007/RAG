WEBVTT

1
00:00:03.222 --> 00:00:14.189
Ritesh Bhagwat: Yes, it recording has started. Okay. So anyone who would. I'm not forcing. I just asking in any ways. I we have slightly diverted from the topic today, anyone who wants to

2
00:00:14.780 --> 00:00:18.148
Ritesh Bhagwat: talk about present from their side about this

3
00:00:18.590 --> 00:00:21.639
Ritesh Bhagwat: what p-value metrics, p-value, and

4
00:00:23.130 --> 00:00:25.849
Ritesh Bhagwat: the the discussion that we we like

5
00:00:26.120 --> 00:00:28.659
Ritesh Bhagwat: we had last time, but did not conclude

6
00:00:28.810 --> 00:00:33.039
Ritesh Bhagwat: for key queries and values metrics. Anyone who would like to present anything?

7
00:00:37.140 --> 00:00:38.200
Ritesh Bhagwat: Zooming. Note.

8
00:00:40.580 --> 00:00:52.789
Ritesh Bhagwat: yeah, yeah. So what happened is, I told you that we were going to discuss that. So what what was happening is we were stuck in a loop. Okay, so what I am thinking. And you let me know. I hope this makes sense to you

9
00:00:53.010 --> 00:01:01.560
Ritesh Bhagwat: is that we'll just park that idea today. The key query value because we have to discuss something else. I mean more on that. Okay.

10
00:01:01.660 --> 00:01:10.999
Ritesh Bhagwat: the problem is, if we get stuck, we'll again get stuck in the same loop today because we are going to. We need to have some hands on to understand that thing. What I planned is.

11
00:01:11.570 --> 00:01:15.020
Ritesh Bhagwat: no, we'll go. We can keep a session a little bit short today.

12
00:01:15.110 --> 00:01:27.950
Ritesh Bhagwat: so we'll get started with Openai. I'll just at least introduce to you how to get started all the stuff that we did with Gemini. You can do with Openai also, we'll just, you know, briefly, get you started so that if you want you can do a hands-on.

13
00:01:28.330 --> 00:01:46.279
Ritesh Bhagwat: Then what we'll do is we will understand. What is this? Bert and Gpt, we get a bird introduction. Okay? All conceptual. Then we'll do. We'll understand what is transfer learning and fine tuning of Llms. Because we wanted to do it from last 2 sessions. But we somehow just couldn't. So this will get things rolling.

14
00:01:46.340 --> 00:02:00.050
Ritesh Bhagwat: and next week what we'll do is we'll do. We'll start the actual fine tuning hands on. So we'll solve problem with Bert. Okay? And at that time we'll come back to key queries and values metrics wherein we will be implementing that simultaneously.

15
00:02:00.290 --> 00:02:12.190
Ritesh Bhagwat: so that if if at all, I'm assuming there is still a confusion in the Kqv. So we will take that Kqv. And integrate it with with our hands on. So you will understand it nicely.

16
00:02:12.730 --> 00:02:14.459
Ritesh Bhagwat: Does that make sense? Very well.

17
00:02:19.325 --> 00:02:19.670
Nishant: Yes.

18
00:02:19.670 --> 00:02:20.230
Aditya: This, show.

19
00:02:20.230 --> 00:02:28.620
Ritesh Bhagwat: Yeah. Yeah. So because otherwise you you get it right, we'll we'll go into the loop again, and we'll keep talking about the theory, and we will just get stuck there.

20
00:02:28.630 --> 00:02:49.659
Ritesh Bhagwat: So you know, otherwise our our course will keep on hanging. It's I mean, I I get a lot of people. I mean, we have, like we did 2014. We are going to in the prospector. It was written 24. We'll probably go to 2829 session at least 4 5 sessions are more remaining. Okay? So we just want to get things going ahead. That's why I thought, you know, doing this would be better.

21
00:02:49.720 --> 00:02:54.529
Ritesh Bhagwat: Yeah, I hope you're all happy and not forced to. Do, you know, do this in this way.

22
00:02:54.940 --> 00:02:58.880
Ritesh Bhagwat: Just need to trust me a little bit on it, I hope? Okay, with everyone. Right?

23
00:02:59.000 --> 00:03:00.230
Ritesh Bhagwat: Happily. Okay. I mean.

24
00:03:00.230 --> 00:03:00.919
Akshay: Yes, it is.

25
00:03:00.920 --> 00:03:01.670
Shubham Gupta: Yes, it is.

26
00:03:01.670 --> 00:03:02.840
Aditya: Yes, itesh right.

27
00:03:02.840 --> 00:03:03.994
Ritesh Bhagwat: Yeah, yeah, okay.

28
00:03:04.990 --> 00:03:09.100
Ritesh Bhagwat: okay, so like I, this will keep it at last. This is a 2 min thing which you have.

29
00:03:09.560 --> 00:03:11.400
Ritesh Bhagwat: They're going to. Okay.

30
00:03:11.480 --> 00:03:14.360
Ritesh Bhagwat: Well, let's talk about something. Who has burnt

31
00:03:15.180 --> 00:03:23.150
Ritesh Bhagwat: before that. I'll just go again to the transformer thing and go transformer architecture.

32
00:03:25.250 --> 00:03:37.099
Ritesh Bhagwat: Okay, so we have. We'll don't worry. We'll see this key, query and everything. You know. I I've already done that. Actually, I've already done that coded that written that in in even a simpler format, but just not

33
00:03:38.110 --> 00:03:41.790
Ritesh Bhagwat: going through it. Now you can think of it like this.

34
00:03:43.880 --> 00:03:50.210
Ritesh Bhagwat: But think of this transformer little bit over simplification, but still it works.

35
00:03:53.430 --> 00:03:56.410
Ritesh Bhagwat: It's like this. Okay, this is our transformer architecture.

36
00:03:57.150 --> 00:04:01.820
Ritesh Bhagwat: If we divide it into 2 parts. Okay, loosely, we divide it into. So this is.

37
00:04:02.360 --> 00:04:08.300
Ritesh Bhagwat: Oh, yeah, let me just take this red part, and I'm just boxing this into a red box

38
00:04:08.830 --> 00:04:17.560
Ritesh Bhagwat: would be cool this red box, and then there is this purple box, Buddy.

39
00:04:17.990 --> 00:04:22.619
Ritesh Bhagwat: to be red? Okay, this is purple box. And now I'm going to take a red box.

40
00:04:24.450 --> 00:04:30.499
Ritesh Bhagwat: Okay, so we can see we have separated it into 2 different boxes. Okay, so the purple one

41
00:04:30.990 --> 00:04:36.340
Ritesh Bhagwat: you can just call it as encoder.

42
00:04:37.380 --> 00:04:40.409
Ritesh Bhagwat: and the red one we can call it as a decoder

43
00:04:42.850 --> 00:04:45.670
Ritesh Bhagwat: decoder. You already know what encoder decoder are right?

44
00:04:46.690 --> 00:04:54.320
Ritesh Bhagwat: So now, what happens is, if we break up this transform into 2 different parts, we can have an encoder, only transformer.

45
00:04:54.620 --> 00:04:59.230
Ritesh Bhagwat: Okay, encoder, only transformer.

46
00:05:02.500 --> 00:05:05.250
Ritesh Bhagwat: and we can have a decoder, only transform

47
00:05:10.070 --> 00:05:11.100
Ritesh Bhagwat: one support.

48
00:05:12.440 --> 00:05:15.750
Ritesh Bhagwat: So what will the encoder? Only transformer? Do any guess

49
00:05:24.750 --> 00:05:27.380
Ritesh Bhagwat: as the name suggests any any guess.

50
00:05:28.390 --> 00:05:37.330
Aditya: It will. I mean, it will take the text, and it will create some kind of a numeric representation, or like it, will do some encoding.

51
00:05:37.580 --> 00:05:52.719
Ritesh Bhagwat: Yes, yes, and we can just transform that encoding again to something. I mean, there would be an encoding for each and every word. So we can just go ahead and form an in form an equivalent word to that. It will not do a language translation. But let's say we have.

52
00:05:53.140 --> 00:05:57.210
Ritesh Bhagwat: We saw in attention, right? That maybe you know.

53
00:05:57.610 --> 00:06:03.609
Ritesh Bhagwat: the bank of the river is the bank of the river is beautiful, right? So it might give a new input into bank.

54
00:06:03.710 --> 00:06:25.010
Ritesh Bhagwat: but it will not. When we decode it right, we want to decode it. We can, if we we saw in our encoder decoder architecture, that we can change this into some language. Right? So we'll encode it, and then we will decode it. So we will write in English and we say, change it to French, but with encoder only transformer, we cannot do that, but we can encode it. I mean, we can update the embeddings.

55
00:06:25.240 --> 00:06:26.839
Ritesh Bhagwat: Does that make sense? I need to.

56
00:06:27.210 --> 00:06:28.490
Aditya: Yes, it is straight. Yeah.

57
00:06:28.490 --> 00:06:38.760
Ritesh Bhagwat: So we can do the attention and all thing, and maybe we can find out whether where the whether the bank is related to this or this that can be done. But in the same language.

58
00:06:40.020 --> 00:06:42.000
Ritesh Bhagwat: right? Is that understood everywhere?

59
00:06:42.220 --> 00:06:47.269
Shrinidhi: So this this doesn't do. Next token prediction. If I use only encoders.

60
00:06:47.270 --> 00:06:54.280
Ritesh Bhagwat: It will do, it will do. I'll tell you how it it does, but it has to be. It is in the same language, it cannot decode it into another language.

61
00:06:55.300 --> 00:06:56.610
Shrinidhi: Okay. Okay.

62
00:06:56.610 --> 00:06:59.939
Ritesh Bhagwat: Okay, if we saw, if you remember, our encoder decoder architecture right

63
00:06:59.950 --> 00:07:23.010
Ritesh Bhagwat: in in that. What we did in that session was we gave something in one language. Then it was like, we started like an intelligent, you know this somebody who knows different languages. Right? I give you an example. That interpreter, that interpreter will take it, convert into embeddings. Right? That is done, and after that it will again go to the decoder and convert it into some other language.

64
00:07:23.380 --> 00:07:28.140
Ritesh Bhagwat: right for English to, or English to French, that cannot be done with encoder only.

65
00:07:29.680 --> 00:07:30.650
Shrinidhi: Okay. Okay.

66
00:07:30.840 --> 00:07:32.290
Ritesh Bhagwat: Okay, does that make sense.

67
00:07:32.620 --> 00:07:33.540
Shrinidhi: Got it? Yeah.

68
00:07:33.540 --> 00:07:40.170
Ritesh Bhagwat: Yeah, yeah. So we will just have encoder, only transformer. And we can actually have decoder, only transformer. Also

69
00:07:40.400 --> 00:07:41.890
Ritesh Bhagwat: the same thing. Right?

70
00:07:42.180 --> 00:07:45.530
Ritesh Bhagwat: Again, can you tell me what decoder only transformer might do.

71
00:07:54.720 --> 00:07:55.690
Shivranjan Kolvankar: Make sure it comes.

72
00:07:55.690 --> 00:08:00.629
Nishant: According to the next, I mean another language, but not in same language.

73
00:08:00.830 --> 00:08:17.139
Ritesh Bhagwat: Okay, so let me let me just tell you something. Okay, that will be sort of again, a little bit of Eureka plus confusion moment. Okay, both of that and which we will see. So the encoder only transformer. There are many types of, but is type of an encoder only. Transformer.

74
00:08:19.370 --> 00:08:20.100
Ritesh Bhagwat: Okay?

75
00:08:20.220 --> 00:08:25.530
Ritesh Bhagwat: And to your surprise, Gpt is a decoder only transform.

76
00:08:30.790 --> 00:08:31.570
Ritesh Bhagwat: Okay.

77
00:08:31.750 --> 00:08:41.410
Ritesh Bhagwat: chat. Gpt, right? Most of it. It is a deco fund. I mean, you can have encoder, decoder, transformer. But generally Gpt is only a decoder, only transform.

78
00:08:41.840 --> 00:08:42.610
Ritesh Bhagwat: Okay.

79
00:08:42.929 --> 00:08:50.410
Ritesh Bhagwat: we'll come to that in subsequent sessions. We are going to discuss all these things. But have you understood the distinction between encoder only and decoder only.

80
00:08:54.790 --> 00:08:55.960
Shrinidhi: Yes, sir. Yes, go ahead.

81
00:08:57.200 --> 00:08:59.070
Nishant: Exactly. Decoder only will do.

82
00:08:59.730 --> 00:09:03.570
Ritesh Bhagwat: Yes, yes, yeah, yeah. Yes. So you were saying something.

83
00:09:03.910 --> 00:09:17.199
Shrinidhi: If it's a decoder only Llm, what will be the input to it like? Because there is no encoder there, I give some text, and it still gets output. Is there some encoding happening, or directly those.

84
00:09:17.200 --> 00:09:22.789
Ritesh Bhagwat: Yes, yes, it happens encoding. It all happens. Okay, we'll see that one by one. So encoding. So basically

85
00:09:23.340 --> 00:09:26.650
Ritesh Bhagwat: question, answer, type of thing we can do with decoder only transfer.

86
00:09:28.830 --> 00:09:29.300
Shrinidhi: Okay.

87
00:09:29.300 --> 00:09:48.219
Ritesh Bhagwat: Okay, we'll see how it happens. Okay, just don't worry. We'll see everything. Today. We are just concentrating to Bird. We will see next. I mean, as we go into fine tuning, we'll see Gpt also how it works. But just for for today's session, discuss that. A lot of things will happen. Okay, it's not just that encoder decoder. But there is something more there is. There is always going to be something.

88
00:09:49.380 --> 00:09:55.800
Ritesh Bhagwat: Okay. Now let me ask you, let me just paraphrase this, can decoder only be used for language translation.

89
00:09:55.810 --> 00:09:58.179
Ritesh Bhagwat: Can't we do some other type of stuff.

90
00:10:01.440 --> 00:10:03.990
Shrinidhi: We are doing everything with chat, gpt.

91
00:10:03.990 --> 00:10:17.429
Ritesh Bhagwat: Yeah. So you you actually, indirectly answered the question. But yeah, if you give an embedding right? If so, what was the input, of decoder only transform in encoder decoder architecture, let's say, embedding in English.

92
00:10:17.560 --> 00:10:22.420
Ritesh Bhagwat: and we asked it to translate it to him Hindi or French. Right?

93
00:10:22.460 --> 00:10:24.619
Ritesh Bhagwat: So encoding was given.

94
00:10:25.510 --> 00:10:26.620
Ritesh Bhagwat: Is that correct?

95
00:10:27.710 --> 00:10:32.130
Ritesh Bhagwat: Let's say some contextual emboding or embedding vector was given to it.

96
00:10:32.640 --> 00:10:33.640
Ritesh Bhagwat: Do you agree.

97
00:10:34.910 --> 00:10:35.710
Shrinidhi: Yes.

98
00:10:35.710 --> 00:10:46.980
Ritesh Bhagwat: Yeah, so just forget about the encoder part just there is no encoder, part exist, but we have already have some embedding, and we have given that embedding to decoder. Then you can ask it to do a few things with that. Those encodings

99
00:10:51.650 --> 00:10:53.489
Ritesh Bhagwat: right? Does does that make sense.

100
00:10:53.490 --> 00:10:54.650
Aditya: Right? Ritesh, yeah.

101
00:10:54.650 --> 00:10:59.839
Ritesh Bhagwat: Yeah, I'm not saying, yeah. So what I'm saying is, just imagine, okay, let me just simplify this.

102
00:11:01.220 --> 00:11:05.410
Ritesh Bhagwat: Let me go ahead and let's say, this is encoder.

103
00:11:08.860 --> 00:11:13.550
Ritesh Bhagwat: this is an encoder, and this is a decoder.

104
00:11:15.640 --> 00:11:19.619
Ritesh Bhagwat: Now we are passing some input here, something here.

105
00:11:19.660 --> 00:11:26.750
Ritesh Bhagwat: Now this encoding is doing some stuff and then giving some contextual embedding with this

106
00:11:27.090 --> 00:11:40.449
Ritesh Bhagwat: right? Let's say it is green, the green, input the green thing is the output of encoder, and then given to the decoder, and then the decoder is doing something. So this is an encoder, architect, encoder, decoder architecture at a high level.

107
00:11:40.540 --> 00:11:41.719
Ritesh Bhagwat: a great synergy.

108
00:11:43.760 --> 00:11:44.720
Shrinidhi: Yes.

109
00:11:44.720 --> 00:11:49.259
Ritesh Bhagwat: Yeah, no. What I'm saying is, they just think that Duke doesn't exist.

110
00:11:49.480 --> 00:11:52.339
Ritesh Bhagwat: and we give some embedding from somewhere else

111
00:11:53.630 --> 00:11:55.940
Ritesh Bhagwat: somewhere else. There is some. ABC, yeah.

112
00:11:56.610 --> 00:11:58.730
Ritesh Bhagwat: just for today, understanding, ABC, is there

113
00:11:58.840 --> 00:12:10.009
Ritesh Bhagwat: still, if we can give some contextual contextual or some type of vectors. Okay, just for today some type of vector we can ask the decoder to do a status, something, something out of these vectors.

114
00:12:11.600 --> 00:12:12.360
Shrinidhi: Yeah.

115
00:12:12.360 --> 00:12:23.160
Ritesh Bhagwat: Yeah, it is expecting something as input, and then it will do its thing right now it can come from here. It can come from an encoder. It can come from anywhere right?

116
00:12:23.280 --> 00:12:32.129
Ritesh Bhagwat: It can come from an encoder, decoder architecture. We give it through an encoder. We pass the information here, we encode it, and then we give it, but we can give it without the encoder. Also.

117
00:12:32.190 --> 00:12:41.359
Ritesh Bhagwat: when we don't use the encoder and give the information from some other stream without and without using the encoder part of the transformer. We call it as a decoder, only transformer.

118
00:12:41.480 --> 00:12:43.199
Ritesh Bhagwat: and that is what Gpt is.

119
00:12:45.180 --> 00:12:45.790
Shrinidhi: Okay.

120
00:12:46.570 --> 00:12:53.549
Ritesh Bhagwat: Okay, I'll tell you next week is all about this. Only when you'll do fine tuning and gpt and all. But today you have to just understand that this can be done.

121
00:12:53.730 --> 00:12:56.819
Ritesh Bhagwat: Okay, now, how it is done. We'll go into that discussion.

122
00:12:56.960 --> 00:12:57.730
Ritesh Bhagwat: Okay.

123
00:12:58.370 --> 00:13:01.820
Shrinidhi: Just one more question like, I'm not sure if I have missed that.

124
00:13:01.830 --> 00:13:02.400
Ritesh Bhagwat: Yeah.

125
00:13:02.400 --> 00:13:13.620
Shrinidhi: Architecturally, what is the difference between the 2? Like encoder and decoder? We we I see there are some linear transformation, Softmax being added in the end, etc. and decoder.

126
00:13:13.690 --> 00:13:18.580
Shrinidhi: but otherwise, if I don't have to get into technical aspect, and

127
00:13:19.090 --> 00:13:22.740
Shrinidhi: so you mean to say, the encoder and decoder part of the transformer.

128
00:13:23.050 --> 00:13:23.629
Ritesh Bhagwat: Or the.

129
00:13:23.630 --> 00:13:31.379
Shrinidhi: Yeah, yeah, I mean what it is actually in the from the output of encoder. What we see is we get just the contextual embeddings. We have an.

130
00:13:31.380 --> 00:13:31.710
Ritesh Bhagwat: That's.

131
00:13:31.710 --> 00:13:37.449
Shrinidhi: Embedding, we add the positional encoding. So the output that gets is the contextual embedding.

132
00:13:37.450 --> 00:13:38.370
Ritesh Bhagwat: Yes, yes.

133
00:13:38.930 --> 00:13:50.739
Shrinidhi: Sorry in in the original transformer. The input embedding also is input to decoder. But along with that we are also passing the contextual embedding, coming out of decoder.

134
00:13:51.300 --> 00:13:55.630
Shrinidhi: But other but other than that.

135
00:13:56.570 --> 00:14:18.700
Shrinidhi: I mean. And then outside, we do some, we get linear. We are doing linear transformation and to get the probabilities of the tokens. We are doing soft, Max, etcetera, but still at the infusion level. I'm I'm I want to know what would be the difference like, when will I know, or when will I decide? I should use encoder or decoder.

136
00:14:19.360 --> 00:14:21.909
Ritesh Bhagwat: Okay. So in this, you're asking about this, right?

137
00:14:22.540 --> 00:14:23.420
Shrinidhi: Yeah.

138
00:14:23.420 --> 00:14:23.830
Ritesh Bhagwat: This way.

139
00:14:23.830 --> 00:14:29.580
Shrinidhi: Individually like, what is the difference like? There I get contextual embedding from encoder.

140
00:14:29.580 --> 00:14:43.489
Ritesh Bhagwat: Yeah. Okay, okay, let me. Yeah. Let me take. Let me take you. Let me give you a very I mean, let me think of something. Yeah, I've just thought something, and I hope it makes sense. Okay. So let me just give you some some analogy, and then we'll come. Okay.

141
00:14:43.860 --> 00:14:46.660
Ritesh Bhagwat: Now, let's say that we have

142
00:14:50.680 --> 00:14:57.860
Ritesh Bhagwat: Okay, I'm just making this analogy in my mind right away. Okay? So it might sound a little vague. Yeah. So let's say we have. This is one part.

143
00:14:58.850 --> 00:14:59.680
Ritesh Bhagwat: then.

144
00:15:06.760 --> 00:15:14.050
Ritesh Bhagwat: okay, so let's say, we are making a vegetable. Okay, I've just put on this analogy. I hope this will make sense. Let's say we want to make some.

145
00:15:15.150 --> 00:15:16.903
Ritesh Bhagwat: What do you say?

146
00:15:17.890 --> 00:15:20.759
Ritesh Bhagwat: veg, Kadai, we all know Veg Kadai, what to Veg Kadai is right.

147
00:15:21.340 --> 00:15:25.489
Ritesh Bhagwat: I mean, we we have eaten. I mean, mixed vegetables. Yeah, fine.

148
00:15:25.600 --> 00:15:29.880
Ritesh Bhagwat: Yeah. So what happens is that we give vegetables.

149
00:15:30.350 --> 00:15:32.330
Ritesh Bhagwat: We give vegetables here.

150
00:15:32.540 --> 00:15:36.520
Ritesh Bhagwat: Now let's say this part, which I'll be just calling it as encoder, for now

151
00:15:36.720 --> 00:15:40.989
Ritesh Bhagwat: what encoder does is it cuts all the vegetables into pieces.

152
00:15:41.670 --> 00:15:44.470
Ritesh Bhagwat: so all the vegetables are cut into pieces.

153
00:15:44.660 --> 00:15:47.230
Ritesh Bhagwat: That is the part of the encoder

154
00:15:47.330 --> 00:15:51.889
Ritesh Bhagwat: vegetable pieces. We cannot do veg pari without cutting it to pieces. Is that correct?

155
00:15:53.210 --> 00:15:53.530
Shrinidhi: Can I please.

156
00:15:53.530 --> 00:16:05.379
Ritesh Bhagwat: Cannot put like full potato and full whatever you know, Benjel and all, we have to cut it into pieces. Okay, so this encoder cuts the vegetable into pieces, and then that cut pieces.

157
00:16:05.720 --> 00:16:16.463
Ritesh Bhagwat: cut pieces, travel to this decoder, and here is where all the gravy is prepared, and then you know all, all stuff the salt is added, and whatever the

158
00:16:17.030 --> 00:16:29.919
Ritesh Bhagwat: preparation. Okay? So that all is done. Okay? So all the preparation, maybe heating, adding of gravy, etcetera, etcetera, right steering it, giving the appropriate temperature that is done, and we get our final

159
00:16:30.190 --> 00:16:33.480
Ritesh Bhagwat: vegetable that can be is ready to eat.

160
00:16:36.770 --> 00:16:39.950
Ritesh Bhagwat: ready to eat.

161
00:16:41.150 --> 00:16:41.760
Shrinidhi: Okay.

162
00:16:41.760 --> 00:16:49.339
Ritesh Bhagwat: Okay, does this make sense? This flow chart? I mean, I it's a flow chart, you know, very vague flow chart. But does this make sense to everyone.

163
00:16:50.640 --> 00:16:51.869
Ritesh Bhagwat: Yeah, it does for me.

164
00:16:52.150 --> 00:16:58.790
Ritesh Bhagwat: So we are doing some basic processing of vegetables in encoder. And then we are preparing vegetables. Okay?

165
00:16:58.940 --> 00:17:03.359
Ritesh Bhagwat: Now, what if I'm just talking about, Bert?

166
00:17:03.620 --> 00:17:09.440
Ritesh Bhagwat: What if we just want to have veg salad, which is a mix of vegetables? And I just want

167
00:17:10.180 --> 00:17:12.060
Ritesh Bhagwat: salt sprinkled on it.

168
00:17:12.680 --> 00:17:13.980
Ritesh Bhagwat: What I'll do is

169
00:17:14.099 --> 00:17:19.859
Ritesh Bhagwat: I will just take this, and rather than giving it to decoder, I'll just take the output here

170
00:17:20.140 --> 00:17:24.250
Ritesh Bhagwat: right. I will just take the output here and sprinkle a little bit of salt on top.

171
00:17:25.329 --> 00:17:31.430
Ritesh Bhagwat: It becomes my veg salad, mixed veg salad, whatever it is mixed. Veg, salad.

172
00:17:31.700 --> 00:17:32.939
Ritesh Bhagwat: Does this make sense.

173
00:17:33.980 --> 00:17:35.119
Shrinidhi: Yeah, it is.

174
00:17:35.420 --> 00:17:50.139
Ritesh Bhagwat: Yeah. Yeah. So we don't need that type of processing for making a salad. We don't need gravy. And we don't need to get it heated, and all that stuff so so there can be a few tasks which are good for encoder. Only

175
00:17:50.420 --> 00:18:01.319
Ritesh Bhagwat: right? There can be a few tasks which is done, and little bit of thing would be needed. You think at least you have to add salt from here, which is okay, right? So we? We don't need to do all this stuff.

176
00:18:01.960 --> 00:18:04.120
Ritesh Bhagwat: We don't need to do all this stuff here.

177
00:18:04.260 --> 00:18:08.010
Ritesh Bhagwat: So now this is an encoder, only transform.

178
00:18:09.160 --> 00:18:16.710
Ritesh Bhagwat: You might laugh at the analogy. But this is actually how it works. Does this make sense before we go to the decoder? Only thing.

179
00:18:17.870 --> 00:18:22.050
Shrinidhi: Yeah, it does, Ritesh. Actually, I'm smiling at this example. It's.

180
00:18:22.050 --> 00:18:23.890
Ritesh Bhagwat: Yeah, yeah, I know everybody's laughing, but.

181
00:18:25.480 --> 00:18:30.560
Shrinidhi: So when this actually is more, what is it? Appealing and sort of ingrained mind? Now.

182
00:18:30.900 --> 00:18:36.930
Ritesh Bhagwat: Yeah. Others also have. You can laugh at me. Others feel free to laugh at me. But I hope it is understood.

183
00:18:37.312 --> 00:18:41.640
Ritesh Bhagwat: Nishant and Akshay Aditya, okay, sure am also joined. True bomb understood right.

184
00:18:43.060 --> 00:18:43.940
Shubham Gupta: Yes, yes.

185
00:18:43.940 --> 00:18:44.389
Aditya: Yes, sir.

186
00:18:44.390 --> 00:18:44.980
Nishant: And.

187
00:18:44.980 --> 00:18:51.399
Ritesh Bhagwat: Yeah, yeah, you can. You feel you can feel free to laugh. Okay, I I will make take make moments. Okay.

188
00:18:51.400 --> 00:18:52.920
Nishant: For example, it will be remembered.

189
00:18:52.940 --> 00:19:02.889
Ritesh Bhagwat: Yeah, it. He will always remember the same example. Okay, now, this is, I'll just, you know, this is like what the encoder only work. Now think of it that we bring

190
00:19:03.980 --> 00:19:14.399
Ritesh Bhagwat: we bring, you know, in in the food, malls and all. We get the vegetables already cut vegetables so we don't need this vegetable cutter. So you bring.

191
00:19:14.780 --> 00:19:21.439
Ritesh Bhagwat: only the already cut vegetables from the Mall, and then those already cut vegetables are now given here.

192
00:19:21.750 --> 00:19:38.909
Ritesh Bhagwat: Now it's up to you whether you want to make Veg Kadai or Veg Andy, or you just want to take you just want to, you know, cut vegetables are here. So you maybe you know those are like mix of cauliflower and cabbage and potato and onion, or you just take onion and delete everything and make an onion, someji

193
00:19:38.910 --> 00:19:53.760
Ritesh Bhagwat: or onion. So you you're not using the encoder to cut the vegetables. You have alternate ways of getting that getting those cut vegetables. So you bring it from there, and then you decide what you want to do with it, whether you want to make mixed veg.

194
00:19:53.760 --> 00:20:03.710
Ritesh Bhagwat: or you just want to make Jira alus. You just take the aloo and add some jira into it. Make wix mail, and so on, and so forth. So that will become a decoder. Only transfer.

195
00:20:04.720 --> 00:20:33.850
Ritesh Bhagwat: Okay, so decoder only might be slightly tricky for you. Gpt thing today, which we'll discuss next week. Don't worry about that, but this is the way both of things can work together. You know, as I told you, we can do all the cutting, and then all the masala, and all also in the in this whole pipeline, or we can have it in 2 distinct ways. One thing does one thing, and if depending on the type of problem we want to solve, we can use an encoder, only transformer. We can use a decoder only transformer.

196
00:20:35.320 --> 00:20:41.339
Ritesh Bhagwat: Okay? So I'm not sure. Was this the question you were trying to ask? Was the question something else.

197
00:20:41.780 --> 00:20:43.740
Shrinidhi: No, it was. It was this question of the.

198
00:20:43.740 --> 00:20:44.580
Ritesh Bhagwat: It's worth discussion.

199
00:20:45.060 --> 00:20:53.080
Ritesh Bhagwat: Yeah, yeah, yeah. So we can have the. The output of this is, we can have a decoder, only transformer. We can have an encoder only transfer, and we can have encoder and decoder only.

200
00:20:54.580 --> 00:21:04.020
Ritesh Bhagwat: Okay. By the way, I you know, I I have to ask this anybody I came to know it. I never thought of it, but came to know it after a lot of time. What is the full form of Gpt.

201
00:21:05.730 --> 00:21:10.540
Ritesh Bhagwat: You also chat. Gpt, right? Chat. Gpt, this. Gpt, that pre-trained transformer.

202
00:21:10.540 --> 00:21:18.450
Ritesh Bhagwat: Yeah. Generative business. Yeah, yeah, that's correct. I you know, I it took me a lot of time Monday I googled it here. What is? Gpt, I used to think

203
00:21:18.590 --> 00:21:25.110
Ritesh Bhagwat: I I used to think this is some general, and this is Transformer PI don't know, but it is generative, pre-trained, transform. Okay.

204
00:21:26.220 --> 00:21:27.259
Ritesh Bhagwat: It came here.

205
00:21:27.260 --> 00:21:28.609
Shrinidhi: I have to move it. Yes.

206
00:21:29.190 --> 00:21:29.980
Ritesh Bhagwat: Buddy.

207
00:21:30.550 --> 00:21:33.950
Shrinidhi: The same here, like even after a very long 3, 4 months into using.

208
00:21:33.950 --> 00:21:41.818
Ritesh Bhagwat: Yes, yes, yeah, yeah. At least we should know the full form. If somebody asked that in the interview it would be embarrassing to not know that.

209
00:21:42.670 --> 00:21:47.069
Ritesh Bhagwat: Okay, yeah. So now, this what this part tells us that

210
00:21:47.470 --> 00:21:53.789
Ritesh Bhagwat: we can have an encoder, only transformer, we can have a decoder only transform. And we can have encoder, decoder, both types of transformer.

211
00:21:54.060 --> 00:22:01.350
Ritesh Bhagwat: Okay, now, Google came up with this encoder. Only transformer known as Bert Board. Okay.

212
00:22:02.190 --> 00:22:04.259
Ritesh Bhagwat: anybody who knows the full form of. But

213
00:22:10.730 --> 00:22:13.210
Ritesh Bhagwat: yeah, it's a long, full form. You can see it on your screen.

214
00:22:13.210 --> 00:22:14.570
Shivranjan Kolvankar: Bi-directional.

215
00:22:14.570 --> 00:22:24.070
Ritesh Bhagwat: Yeah, yeah, by direct. I also just remember bidirectional encoder. Okay, so it is bidirectional encoder representations from transformers.

216
00:22:24.230 --> 00:22:25.100
Ritesh Bhagwat: That is important.

217
00:22:25.220 --> 00:22:26.600
Shivranjan Kolvankar: It's not.

218
00:22:27.540 --> 00:22:28.230
Ritesh Bhagwat: Sorry.

219
00:22:28.600 --> 00:22:32.740
Shivranjan Kolvankar: I was gonna say text, I remember the representation. I was not.

220
00:22:33.160 --> 00:22:43.679
Ritesh Bhagwat: Okay, okay, okay, I just remember bidirectional encoder only. But it is. But it is just known as bird. This is a bird transformer, which is actually an encoder. Only transformer. Okay.

221
00:22:43.800 --> 00:22:49.149
Ritesh Bhagwat: it is an encoder, only transformer. And by default it does only 2 things.

222
00:22:49.870 --> 00:22:52.630
Ritesh Bhagwat: Let me just copy this, and we'll read it. Okay.

223
00:23:01.780 --> 00:23:11.259
Ritesh Bhagwat: Now we will go into the, you know, crux, of what fine tuning is, and why fine tuning is important, and then we'll when we do fine tuning. Next week we will go to Kqkw

224
00:23:12.020 --> 00:23:21.099
Ritesh Bhagwat: addresses. Okay? So by direction, transformer is a transformer model based on based by Google for national. It uses bi-directional approach, meaning it.

225
00:23:21.110 --> 00:23:24.230
Ritesh Bhagwat: But yeah, this is important. Can you somebody explain to me what this is?

226
00:23:37.540 --> 00:23:38.050
Ritesh Bhagwat: You see.

227
00:23:42.570 --> 00:23:46.210
Ritesh Bhagwat: Yeah. It is like word 2 way, but much more powerful than word. 2 way.

228
00:23:47.450 --> 00:23:51.679
Shubham Gupta: Like it is trying to calculate the distance from both the 2.

229
00:23:51.680 --> 00:24:05.520
Ritesh Bhagwat: Yes, yes, yes, yes, very good, very good. So what it does is, it was the 1st thing that was bidirectional in nature. It takes into consideration. And it is, you know, you can have a lot context level. So it's not just 2 words and 3 words.

230
00:24:05.560 --> 00:24:10.020
Ritesh Bhagwat: It takes into consideration what was written earlier and what was written later.

231
00:24:10.390 --> 00:24:29.289
Ritesh Bhagwat: Then it comes up with with, you know, with the answer that are expected. So it is bidirectional. So now it is. Now we take this bidirectional thing taken for granted. But I think this was somewhere around 215. If I'm not sure. It was somewhere around a decade ago, where, by direction, was a huge thing right at that point of time

232
00:24:29.580 --> 00:24:32.369
Ritesh Bhagwat: this was this was even pre-transformers.

233
00:24:32.780 --> 00:24:41.440
Ritesh Bhagwat: The pre pre transformer thing. So, having context from both the signs together is a huge thing. Is that correct for the machine to understand that.

234
00:24:42.960 --> 00:24:43.730
Akshay: Yes.

235
00:24:43.980 --> 00:24:48.969
Ritesh Bhagwat: Yeah, yeah. So it was quite huge. There, now, this was just used for 2 tasks only

236
00:24:49.720 --> 00:24:57.280
Ritesh Bhagwat: it was used for 2 tasks that you know, we you remember this, fill in the blanks, fill in the blanks. Type of things we used to get in the words.

237
00:24:57.460 --> 00:24:58.700
Ritesh Bhagwat: a primary school

238
00:24:59.325 --> 00:25:04.469
Ritesh Bhagwat: fill in the blank step of exercise in our in our examination. I hope you all remember.

239
00:25:05.980 --> 00:25:08.820
Akshay: Yes, one more staying.

240
00:25:09.140 --> 00:25:12.100
Ritesh Bhagwat: Sorry, Akshay, your voice was breaking. Please come on.

241
00:25:15.200 --> 00:25:16.310
Akshay: Said yes.

242
00:25:16.740 --> 00:25:21.889
Ritesh Bhagwat: Yeah, yeah. Okay. So others also, remember, right? Now, it does the fill in the blanks type of stuff.

243
00:25:21.950 --> 00:25:27.880
Ritesh Bhagwat: Okay? But we'll see when we solve the birth problem, we'll go into the deep architecture of it. But it does 2 types of work.

244
00:25:28.050 --> 00:25:47.849
Ritesh Bhagwat: It actually goes and fill in the blank. So you give a sentence, and you mask some word from the sentence so based on the context what was used before a few words before a few words. Afterwards it will tell you what should come in that word. So random word in the sentence are masked and the model predicts the them based on the context.

245
00:25:48.760 --> 00:25:50.659
Ritesh Bhagwat: Okay, have you understood this point?

246
00:25:50.680 --> 00:25:51.819
Ritesh Bhagwat: Point number one.

247
00:25:54.750 --> 00:25:55.489
Aditya: Ish, yeah.

248
00:25:55.490 --> 00:25:58.190
Ritesh Bhagwat: Yeah, yeah. So this was the default thing.

249
00:25:58.220 --> 00:26:02.189
Ritesh Bhagwat: And the second thing was next sentence, prediction.

250
00:26:02.520 --> 00:26:13.259
Ritesh Bhagwat: So it determines if what if one sentence logically follows the other. So you give it one sentence and what it? What would come later out after that sentence, it can do that as

251
00:26:16.060 --> 00:26:19.689
Ritesh Bhagwat: okay. So these were the only 2 tasks that it used to perform.

252
00:26:20.050 --> 00:26:48.330
Ritesh Bhagwat: That is, one is fill in the blanks. I mean it is. It is called mask language, modeling in a specific term, I just call it fill in the blanks. Okay? So fill in the blanks. And next sentence prediction. So this, these, so how it does, and how we will do it in real time, and how we'll solve problems with it. We'll do in our next session, where we'll do the Kqw matrices as well as this bus reward transformation. Okay, so these are the 2 things that it does nicely, or only these 2 things.

253
00:26:48.940 --> 00:26:51.970
Ritesh Bhagwat: Okay, does it make any questions on this.

254
00:26:54.750 --> 00:26:55.410
Shrinidhi: No.

255
00:26:55.970 --> 00:27:03.850
Ritesh Bhagwat: Now it is a great model to solve classification problems.

256
00:27:05.910 --> 00:27:08.380
Ritesh Bhagwat: Now is the crux. Now is the interesting part.

257
00:27:08.600 --> 00:27:13.789
Ritesh Bhagwat: So this can be gate which we which we are also going to solve next week. Okay, next week is dedicated to Burt only.

258
00:27:13.950 --> 00:27:19.300
Ritesh Bhagwat: So we can solve all classification problems through this port model?

259
00:27:19.970 --> 00:27:20.720
Ritesh Bhagwat: No.

260
00:27:21.670 --> 00:27:26.530
Ritesh Bhagwat: Does any one of these 2 things that it does looks like a classification problem

261
00:27:26.710 --> 00:27:49.560
Ritesh Bhagwat: like if you want, I mean, classification is like, you know, if you give it tweet sentiment to it and ask it whether it is positive negative. It would do all the classification types, you know, if you give it. A student reviews, whether the review is positive, negative, or neutral, or harsh, or very harsh, or very status, or sarcastic, etc, etc, etc, all the types of multi classification as well as binary classification.

262
00:27:50.230 --> 00:27:53.449
Ritesh Bhagwat: Does any one of the these 2 tasks look

263
00:27:53.940 --> 00:27:56.150
Ritesh Bhagwat: like a classification problem to you?

264
00:27:58.514 --> 00:28:03.829
Shrinidhi: The second one. Next sentence, prediction, if one sentence is logically follows the other one.

265
00:28:04.080 --> 00:28:06.980
Ritesh Bhagwat: Okay, so is it classification or generation.

266
00:28:08.800 --> 00:28:10.430
Nishant: It will be generation.

267
00:28:10.680 --> 00:28:11.790
Ritesh Bhagwat: Generation, right.

268
00:28:13.630 --> 00:28:14.420
Shrinidhi: Okay.

269
00:28:14.840 --> 00:28:17.799
Ritesh Bhagwat: And classification. Let's say we have 10 classes.

270
00:28:19.540 --> 00:28:25.049
Ritesh Bhagwat: Okay? So it it should not predict anything other than 10 classes, but a sentence. It can do anything right

271
00:28:26.270 --> 00:28:30.530
Ritesh Bhagwat: based on this sentence, we cannot restrict it that only these 10 words will come

272
00:28:30.570 --> 00:28:32.569
Ritesh Bhagwat: based on this context, it can

273
00:28:32.660 --> 00:28:39.190
Ritesh Bhagwat: technically quote unquote technically generate whatever it thinks is a logical flow. But we are not classifying it.

274
00:28:39.480 --> 00:28:40.650
Ritesh Bhagwat: Does that make sense.

275
00:28:42.220 --> 00:28:42.900
Shrinidhi: Yes.

276
00:28:43.040 --> 00:28:54.749
Ritesh Bhagwat: In the. If you, if we can consider that as classification, then even the fill in the blanks is the classification right? If let's say this, this is mass, this sentence is mass, it is just predicting the next word. It is classifying what it called

277
00:28:55.070 --> 00:28:58.139
Ritesh Bhagwat: so fundamentally, everything becomes a classification type of thing.

278
00:28:58.150 --> 00:29:07.430
Ritesh Bhagwat: But I'm just talking about the the generic idea of how classification problems work. So these 2 don't look like classification. Would you agree? Sweeney and others.

279
00:29:07.430 --> 00:29:08.320
Shrinidhi: Yeah, yeah.

280
00:29:08.550 --> 00:29:09.250
Ritesh Bhagwat: Yep. Yep.

281
00:29:09.630 --> 00:29:09.989
Aditya: Yes, sir.

282
00:29:09.990 --> 00:29:19.159
Ritesh Bhagwat: But yeah, but we can do wonder. We can solve great classification problems with but in fact, after. But everybody just use it, but only to do the classification problem.

283
00:29:19.220 --> 00:29:21.449
Ritesh Bhagwat: Now, how can we do that? Any guess?

284
00:29:24.300 --> 00:29:27.900
Ritesh Bhagwat: I'm not asking you architecture or technical question, or anything.

285
00:29:27.950 --> 00:29:29.179
Ritesh Bhagwat: I'm just asking you.

286
00:29:29.400 --> 00:29:30.719
Ritesh Bhagwat: What do you feel?

287
00:29:31.140 --> 00:29:37.899
Ritesh Bhagwat: How can we do that if it it is just able to do these 2 things? And we have to make it do something. 3, rd which is not

288
00:29:38.794 --> 00:29:43.059
Ritesh Bhagwat: related to these 2 things. Then how can we do that?

289
00:29:47.940 --> 00:29:58.420
Shrinidhi: Since you told that Bert is only an encoder model. The output embedding we get, if we add a softmax layer in the end. When that give us the problem.

290
00:29:58.420 --> 00:30:06.900
Ritesh Bhagwat: Very good. So you are. Yeah. So you told that is the thing that you do. So what we'll do we'll tweak the output a little bit. Is that correct?

291
00:30:08.130 --> 00:30:08.870
Shrinidhi: Yeah.

292
00:30:09.100 --> 00:30:10.889
Ritesh Bhagwat: Yeah, yeah, does everyone agree?

293
00:30:10.930 --> 00:30:13.239
Ritesh Bhagwat: We can tweak the output a little bit.

294
00:30:13.500 --> 00:30:14.270
Akshay: Yes.

295
00:30:14.550 --> 00:30:16.730
Ritesh Bhagwat: Yeah. So that is what fine tuning is all about.

296
00:30:18.120 --> 00:30:20.050
Ritesh Bhagwat: That is what fine tuning is.

297
00:30:20.660 --> 00:30:28.970
Ritesh Bhagwat: Now, it's a very generic idea which will which we'll discuss it a great length like for 1520 min. Just understanding what fine tunings.

298
00:30:29.090 --> 00:30:46.220
Ritesh Bhagwat: Okay, you have to understand this next week on our only fine tuning is remaining remaining, and you know, chat gpt, and we'll do. You know whatever we did with Gemini, we'll do it with Chat Gpt, and also anthropic cloud. But the crux, 6. Time will load a lot of hugging fix transformers. Fine tuning. So we have to just understand what fine tuning is.

299
00:30:46.810 --> 00:30:53.220
Ritesh Bhagwat: Okay. Now with this idea that I just told you we are tweaking something outside. This is fine tuning.

300
00:30:53.690 --> 00:30:57.470
Ritesh Bhagwat: So now, can you give me? Can you just think of something.

301
00:30:57.970 --> 00:31:04.460
Ritesh Bhagwat: use cases, or some some some something that you that comes to your mind that you can do with fine tune.

302
00:31:05.430 --> 00:31:07.970
Ritesh Bhagwat: But what eventually fine-tuning is.

303
00:31:09.480 --> 00:31:37.939
Ritesh Bhagwat: what are the things you can do with fine tuning? What are the ways? I'm not asking any technical thing. I'm just asking. Very general thing can give any analogies also. Wherein you can, you know. Go ahead and you know. Use this fine tuning, or how can we? What does fine tuning mean to you? Let me just put it this way. Let me ask you a simple question, what does fine tuning mean to you with the limited amount of knowledge that we have right now that we can tweak the output a little bit.

304
00:31:42.740 --> 00:31:44.399
Ritesh Bhagwat: Okay? Have you understood the question?

305
00:31:47.340 --> 00:31:48.400
Shrinidhi: You.

306
00:31:48.400 --> 00:31:52.940
Ritesh Bhagwat: Yeah, yeah. Just think through it. If long. Answer, also, fine as always. Don't worry.

307
00:31:53.040 --> 00:31:54.210
Ritesh Bhagwat: You're talking slow.

308
00:31:54.210 --> 00:32:00.949
Aditya: So this, like it, is like taking a pre-trained model and adapting it for some specific tasks which

309
00:32:01.330 --> 00:32:02.190
Aditya: to achieve.

310
00:32:02.320 --> 00:32:05.020
Ritesh Bhagwat: Very good, very good. Yep, yep, exactly.

311
00:32:05.050 --> 00:32:07.580
Ritesh Bhagwat: You take a model and adapt it to a

312
00:32:07.904 --> 00:32:17.909
Ritesh Bhagwat: predict. Okay? So the the 1st thing I. Always, we have always talked about this. We'll talk it about that's today. So let's say, we want to build a model which identifies zebra

313
00:32:18.110 --> 00:32:19.110
Ritesh Bhagwat: be brass.

314
00:32:20.840 --> 00:32:25.189
Ritesh Bhagwat: Now we already have a great model which is good at identifying odds.

315
00:32:26.980 --> 00:32:28.990
Ritesh Bhagwat: Can we use this model for zebra?

316
00:32:34.130 --> 00:32:37.279
Ritesh Bhagwat: If no, why not? If yes, why and how.

317
00:32:41.780 --> 00:32:43.970
Nishant: We need to fine tune it for Debra.

318
00:32:44.490 --> 00:32:45.750
Ritesh Bhagwat: Okay, yes.

319
00:32:45.990 --> 00:32:47.550
Ritesh Bhagwat: So we can use it right?

320
00:32:48.450 --> 00:32:49.080
Nishant: Yeah.

321
00:32:49.270 --> 00:32:51.520
Ritesh Bhagwat: Yeah. And why do we say that we can use it.

322
00:32:53.530 --> 00:32:55.279
Nishant: Your basic structure is same.

323
00:32:55.280 --> 00:32:57.990
Ritesh Bhagwat: Yeah, yeah, does everyone agree with Nishawn?

324
00:33:00.030 --> 00:33:18.499
Ritesh Bhagwat: Basic structure? Is it right? Right? So instead of instead of starting something from scratch and making it, you know, making it learn the structure and all. We will do this horse, and we will take this transformer and we'll train it with big models of zebras right?

325
00:33:18.650 --> 00:33:27.209
Ritesh Bhagwat: And then it will understand that something which is like the horse. But there are stripes on the body is a zebra. That is what all is needed for the model to understand

326
00:33:28.170 --> 00:33:53.100
Ritesh Bhagwat: right? This is known as transfer learning. By the way, I've shared the I've already shared the Pre this recording with you in the recorded session. This is known as transfer learning. When we take one system, and we, instead of building something from scratch. We just go ahead and and you we go ahead. And you know, tweak with that and make it make it more useful for our use case. And that is what transfer learning is

327
00:33:53.480 --> 00:33:54.820
Ritesh Bhagwat: okay. Is this clear.

328
00:33:57.430 --> 00:33:58.210
Shrinidhi: Yep.

329
00:33:58.390 --> 00:34:04.550
Ritesh Bhagwat: Yeah. So let me give you an example. Okay, let's say that we have a neural network. I'm not talking about transformer. Okay, I'm just talking about a neural network.

330
00:34:04.570 --> 00:34:06.359
Ritesh Bhagwat: Let's say, we have a neural network.

331
00:34:06.550 --> 00:34:13.570
Ritesh Bhagwat: This is input, these are pre trained network. By the way, and we have a few hidden layers. Okay, so I'm just taking one.

332
00:34:15.710 --> 00:34:20.060
Ritesh Bhagwat: the 6.

333
00:34:20.820 --> 00:34:23.500
Ritesh Bhagwat: And then this is 7.

334
00:34:24.780 --> 00:34:30.250
Ritesh Bhagwat: And then we will obviously have weights and biases and all neurons here, all neurons. Here.

335
00:34:32.969 --> 00:34:35.020
Ritesh Bhagwat: take me wrong, is it you've done?

336
00:34:35.449 --> 00:34:40.619
Ritesh Bhagwat: What we do is this is a fully connected network. Everything is connected to everything. This.

337
00:34:40.860 --> 00:34:42.090
Ritesh Bhagwat: Yes, yeah.

338
00:34:46.460 --> 00:34:50.019
Ritesh Bhagwat: And here. And let's say this is tells us whether

339
00:34:50.040 --> 00:34:52.480
Ritesh Bhagwat: this is a false or a doubt.

340
00:34:52.770 --> 00:34:56.520
Ritesh Bhagwat: This model tells us whether the image is of a horse or a dog.

341
00:34:57.400 --> 00:35:03.589
Ritesh Bhagwat: And this is a very good, it's there. And it's it's a already solved problem, it is. It is doing great.

342
00:35:04.620 --> 00:35:05.809
Ritesh Bhagwat: He's doing great.

343
00:35:07.610 --> 00:35:10.900
Ritesh Bhagwat: Okay, any questions on this. This is fine understood.

344
00:35:15.380 --> 00:35:16.330
Aditya: Yes, Ritesh, thank you.

345
00:35:16.330 --> 00:35:26.179
Ritesh Bhagwat: Yes. Now, now, what we want to do is we want to build a new network which identifies whether the image given to it of an animal is of a horse.

346
00:35:26.290 --> 00:35:28.420
Ritesh Bhagwat: donkey or zibar.

347
00:35:29.880 --> 00:35:31.970
Ritesh Bhagwat: Okay, so what will you do here?

348
00:35:35.280 --> 00:35:36.700
Ritesh Bhagwat: Have you understood the question?

349
00:35:43.350 --> 00:35:50.080
Ritesh Bhagwat: Yeah, we want to find a problem. We want to find a we want to build a network which identifies horse donkey and zebra.

350
00:35:50.200 --> 00:35:54.760
Ritesh Bhagwat: So what will be your strategy to build this model.

351
00:36:06.729 --> 00:36:09.150
Aditya: I think we need. We'll need to add the

352
00:36:09.772 --> 00:36:13.449
Aditya: like. In the final year. We'll need to add, you know, for 3 categories.

353
00:36:13.450 --> 00:36:14.990
Ritesh Bhagwat: Very good, very ruins.

354
00:36:14.990 --> 00:36:17.399
Ritesh Bhagwat: What we are going to give is we are going to delete this.

355
00:36:19.080 --> 00:36:20.149
Ritesh Bhagwat: Is that correct?

356
00:36:21.030 --> 00:36:21.620
Nishant: Pretty.

357
00:36:22.190 --> 00:36:22.740
Ritesh Bhagwat: Yeah.

358
00:36:23.030 --> 00:36:31.859
Ritesh Bhagwat: And one is, you just go ahead and we go ahead and add a multi-classification layer here.

359
00:36:33.670 --> 00:36:37.299
Ritesh Bhagwat: Right? And this is like horse. This is donkey, and this is zebra.

360
00:36:38.260 --> 00:36:48.269
Ritesh Bhagwat: right? This is the one thing we can do right. But assuming that our day, our this problem is going to be a little bit more complex. Do you agree? Identify horse and donkey.

361
00:36:48.410 --> 00:36:57.489
Ritesh Bhagwat: and identify horse, zebra and donkey? Then how does horse zebra and donkey identification is a little bit more complex than only horse and donkey. Does everyone agree to that?

362
00:36:58.540 --> 00:36:59.120
Shrinidhi: Yep.

363
00:36:59.510 --> 00:37:06.580
Ritesh Bhagwat: Yeah. So what we are going to do just be before doing this, maybe we will. We will say that I'm going to add a few more layers.

364
00:37:08.060 --> 00:37:10.519
Ritesh Bhagwat: we will add a few more layers. Here.

365
00:37:12.440 --> 00:37:17.800
Ritesh Bhagwat: Let's say we have decided to add 2 more layers, and then what we are going to do is

366
00:37:17.900 --> 00:37:21.670
Ritesh Bhagwat: we're going to make it finally a multi-classification.

367
00:37:22.950 --> 00:37:25.119
Ritesh Bhagwat: this one, this one and this one

368
00:37:26.450 --> 00:37:33.539
Ritesh Bhagwat: correct. Now, what we have done, we have taken an existing model and made it tweaked it to our understanding

369
00:37:33.840 --> 00:37:39.090
Ritesh Bhagwat: right? And we just call it a transfer learn which is actually a super set of point. And I'm coming to that

370
00:37:39.240 --> 00:37:40.160
Ritesh Bhagwat: now.

371
00:37:40.600 --> 00:37:50.879
Ritesh Bhagwat: Now, what what is going to happen is when we go ahead. When we go ahead, and we we will give. Now what we'll give. We'll give horse donkeys and zebra

372
00:37:51.206 --> 00:37:56.940
Ritesh Bhagwat: images to this model, right? And it will start and we'll train the model with new images. Is that correct?

373
00:37:59.750 --> 00:38:00.540
Shrinidhi: Yep.

374
00:38:00.540 --> 00:38:05.589
Ritesh Bhagwat: Yep. Now, what is going to happen when we are going to train this with 2 images

375
00:38:05.730 --> 00:38:07.789
Ritesh Bhagwat: in the neural network? What is going to happen.

376
00:38:13.210 --> 00:38:15.610
Shrinidhi: So now update the weights as per the new.

377
00:38:15.610 --> 00:38:21.010
Ritesh Bhagwat: And weights and bias. Right? So the entire weights and biases will be updated. Is that correct?

378
00:38:22.730 --> 00:38:24.699
Ritesh Bhagwat: Yes, additive option.

379
00:38:24.700 --> 00:38:25.720
Aditya: Yes, ritesh right.

380
00:38:26.050 --> 00:38:33.109
Ritesh Bhagwat: Now the problem happens here is that I'm just talking about transfer learning for a minute, and we'll equate it to our Nlp and transformers in a minute.

381
00:38:33.220 --> 00:38:51.569
Ritesh Bhagwat: Now what happens is in a neural network. The initial layer always initial layers are always they. They study the, you know, outlines of the things. Then the middle layer studies the details of the out more detailed and the outer layer studies, the detail, more details of the details. I think we have spoken about it in our previous sessions, right

382
00:38:53.940 --> 00:38:58.800
Ritesh Bhagwat: or not. If not, then we'll speak it right now, or, if you don't remember, we can speak it right now.

383
00:39:02.780 --> 00:39:04.030
Ritesh Bhagwat: Yeah. Please tell me.

384
00:39:05.260 --> 00:39:09.100
Nishant: Yeah, we have spoken. I guess we can log the layer. Well.

385
00:39:09.280 --> 00:39:10.050
Ritesh Bhagwat: Yeah, yeah, yeah.

386
00:39:10.850 --> 00:39:16.989
Ritesh Bhagwat: Yes, yes. So, Aditya, do you remember? Or do you want me, Shubam? Do you want me to discuss this quickly?

387
00:39:18.640 --> 00:39:22.979
Aditya: Yeah, Ritesh, I somewhat remember, like, I think, we discussed this in one of the sessions.

388
00:39:22.980 --> 00:39:37.049
Ritesh Bhagwat: Yes, yes. So the initial layers are always the the it it they basically structure. Study the outlines that the middle layer study the more details, the outer the layer, the more detailed the studies. Right? So if you will go ahead and

389
00:39:37.350 --> 00:39:42.510
Ritesh Bhagwat: just playing it like this, all the weights and biases will be updated. So the initial layers.

390
00:39:42.670 --> 00:40:06.349
Ritesh Bhagwat: because a zebra, the basic outline of a zebra is exactly like a horse and donkey. So we don't want to disturb the outline of this layer. Let's say these these 3 layers. Okay, these 3 layers are these 3 layers are important because they have. They know the outline. So we don't want to disturb those weights and biases, because that has a lot of information of the basic structure. What we do is we freeze these layers.

391
00:40:06.930 --> 00:40:19.000
Ritesh Bhagwat: we freeze these layers and then train. So now what will happen in the 1st 3 layers? The weight and biases will always remain the same. What was from the original model and the weights and biases from the 4th layers will start to update.

392
00:40:19.570 --> 00:40:21.090
Ritesh Bhagwat: Okay, does that make sense.

393
00:40:23.980 --> 00:40:24.690
Shrinidhi: Yep.

394
00:40:24.990 --> 00:40:29.920
Ritesh Bhagwat: Yep, yep, now, Aditya, I'm coming to something that you mentioned in last session, or last to last session.

395
00:40:30.350 --> 00:40:32.579
Ritesh Bhagwat: Now, when we talk about

396
00:40:33.850 --> 00:40:58.240
Ritesh Bhagwat: our nlp or language, right? Our language or etc, whatever it is. Right. So of course, there is weights and biases. Everything is weights and biases. But there is something more than weights and biases that is going to change. If we let's say we have a transformer, which is very good at doing something which is like bird, and we want to make it as a classification transformer. So we add some tasks related to classification, and we start training it.

397
00:40:58.340 --> 00:41:00.060
Ritesh Bhagwat: What is going to happen?

398
00:41:01.180 --> 00:41:07.699
Ritesh Bhagwat: Weights and biases, there is always a neural network. The weights and biases is going to change. Something else is going to change and

399
00:41:07.790 --> 00:41:13.140
Ritesh Bhagwat: adity actually mentioned it in one of the sessions. I don't remember if if it was last session or last to last session.

400
00:41:17.820 --> 00:41:19.329
Nishant: What else feature.

401
00:41:19.510 --> 00:41:20.330
Ritesh Bhagwat: Sorry.

402
00:41:21.270 --> 00:41:22.470
Nishant: Loss will change.

403
00:41:22.870 --> 00:41:32.079
Ritesh Bhagwat: No, no losses. Loss is going to loss is always going to change right? It is going to reduce. The more we train one. There is one specific thing when we talk about

404
00:41:32.110 --> 00:41:36.719
Ritesh Bhagwat: or transformers and Nlp which is not present in this type, Cnn type of network.

405
00:41:37.290 --> 00:41:37.850
Nishant: Okay.

406
00:41:39.540 --> 00:41:44.790
Ritesh Bhagwat: We have been speaking. Let me give you a head. We have been speaking this in almost all our previous sessions.

407
00:41:44.880 --> 00:41:47.630
Ritesh Bhagwat: but since we started since we started in it.

408
00:41:48.060 --> 00:41:54.939
Ritesh Bhagwat: And Aditya actually said this in one of the sessions. I remember distinctly that he says fine tuning means changing this.

409
00:42:01.830 --> 00:42:02.969
Shubham Gupta: The expense.

410
00:42:03.310 --> 00:42:05.530
Shrinidhi: Changing the contact. Embeddings.

411
00:42:05.750 --> 00:42:09.910
Ritesh Bhagwat: Embeddings exact exactly. The embeddings is going to change right

412
00:42:11.540 --> 00:42:23.720
Ritesh Bhagwat: if we go ahead and model model is trained, and we stop there model in in terms of Nlp. We have our weights and biases, and we have our final embeddings. Now, if we will go ahead and train it more, the embeddings are going to change.

413
00:42:24.750 --> 00:42:25.850
Ritesh Bhagwat: Is that correct?

414
00:42:30.130 --> 00:42:30.830
Shrinidhi: Yep.

415
00:42:31.060 --> 00:42:32.990
Ritesh Bhagwat: Yeah. Yeah. So, Aditya, do you remember

416
00:42:33.040 --> 00:42:38.520
Ritesh Bhagwat: you said that fine tuning means changing of embeddings yesterday? Right? Yeah. I remember.

417
00:42:38.520 --> 00:43:00.180
Ritesh Bhagwat: I think you already said it. I I distinctly remember that. And I told you I will come back to that. Yeah. So same thing in when we talk about languages and all our embeddings are going to change right now, for example, this, this works well with the transformer thing we studied right, for example, the bank thing a bank will have a single embedding, but based on attention whenever we do attending. If it is used with river.

418
00:43:00.180 --> 00:43:11.270
Ritesh Bhagwat: the embedding of the bank changes, if if it uses with financial institution the embedding of the bank changes. We already discussed that. Okay, so now, what happens with fine tuning is fine. Tuning is a very, very huge umbrella.

419
00:43:11.450 --> 00:43:23.750
Ritesh Bhagwat: So let's say that your model. Okay, I'm just not talking about world right now. Just transformer. Just, I'm talking about you know the entire transformer thing. Let's say your transformer is very good at

420
00:43:24.360 --> 00:43:32.139
Ritesh Bhagwat: question, answers question and answers great. It is very good at question answers.

421
00:43:32.270 --> 00:43:40.370
Ritesh Bhagwat: So it's a very generic transformer. You can ask its question answers for anything under the sun. Okay, anything. But let's say you are a bank.

422
00:43:41.120 --> 00:43:42.710
Ritesh Bhagwat: Okay? Or you're a hospital.

423
00:43:42.970 --> 00:43:43.730
Ritesh Bhagwat: Okay?

424
00:43:43.850 --> 00:43:48.330
Ritesh Bhagwat: So if you're a hospital and you want to build something for a hospital, do you need?

425
00:43:48.630 --> 00:43:54.870
Ritesh Bhagwat: Do you need anything related to? Let's say cooking or astrology, or

426
00:43:54.920 --> 00:44:08.279
Ritesh Bhagwat: maybe you know something related to astronomy. Will any of the patients that are in you know, want to do some you know interact with your transformer. Are they going to ask the hospital about cooking or anything like that?

427
00:44:13.090 --> 00:44:18.650
Ritesh Bhagwat: Yeah, tell me if you want to inquire something about a hospital. There is a chat. Bot. Will you inquire? Cooking instructions from a hospital.

428
00:44:19.596 --> 00:44:20.349
Aditya: Northish, no.

429
00:44:20.350 --> 00:44:29.619
Ritesh Bhagwat: You will never do that right. But this this transformer, which is a transformer, it is Qa, which is like very generic purpose. Transformer. You can ask anything to it

430
00:44:29.720 --> 00:44:34.680
Ritesh Bhagwat: now, if I want to make it a hospital specific transformer, what should I do.

431
00:44:39.000 --> 00:44:43.289
Shrinidhi: I have to fine tune with only hospital domain specific data.

432
00:44:43.290 --> 00:44:51.060
Ritesh Bhagwat: Exactly exactly while preserving all the intelligence that it has learned how to how to go ahead and do a question. Answer

433
00:44:51.180 --> 00:44:57.470
Ritesh Bhagwat: the basic, the basic con or the basic structure of con question. Answer will always remain the same

434
00:44:57.780 --> 00:45:08.019
Ritesh Bhagwat: right? And so it has understood the language context. Can I say that the generic generic purpose transformer has understood the language context.

435
00:45:08.270 --> 00:45:09.919
Ritesh Bhagwat: does that make sense?

436
00:45:12.200 --> 00:45:18.820
Ritesh Bhagwat: Yeah, you can, you can question, answer it on any topic. So it understands how the contextual language works. Can I say that.

437
00:45:19.530 --> 00:45:23.320
Aditya: Yes, I mean it. It has a very good understanding of the language.

438
00:45:23.320 --> 00:45:24.640
Ritesh Bhagwat: Yes, yes, exactly.

439
00:45:24.640 --> 00:45:25.140
Aditya: So.

440
00:45:25.560 --> 00:45:46.809
Ritesh Bhagwat: Yeah. Yeah. So what what we will do is we will fine tune it. What we'll do. We'll just bring a new data set, which is very hospital specific data set. Okay? And we will tweak a trans that transformer a little bit and make it very, very hospital specific. So it becomes a domain specific, transformer, right domain specific. And this is all. This is also a part of fine tuning

441
00:45:47.090 --> 00:46:14.169
Ritesh Bhagwat: that you take a very powerful thing and make it even more powerful in a sub subset area. Now, the thing is, if you don't do this, if you let's say I'm taking this orange box, if if you don't do this, what is inside the orange box, and you just ask hospital questions to it, it will still answer it, but once you train it with specific hospital data. It will, it will do it in a much more better way, because it is trained specific for that task, and not a generic task.

442
00:46:14.590 --> 00:46:15.790
Ritesh Bhagwat: Does that make sense?

443
00:46:23.020 --> 00:46:24.169
Ritesh Bhagwat: Yeah, yes or no?

444
00:46:24.170 --> 00:46:25.260
Aditya: Yesterday, straight.

445
00:46:25.260 --> 00:46:33.979
Ritesh Bhagwat: Yeah. So do you see, let me ask you a counter question. Do you see any benefit in this, that, rather than using a generic thing which also answers.

446
00:46:34.977 --> 00:46:41.330
Ritesh Bhagwat: hospital related questions. You fine, tune it with your data set and make it hospital specific.

447
00:46:41.490 --> 00:46:43.399
Ritesh Bhagwat: Do you see any benefit in this.

448
00:46:49.840 --> 00:46:59.798
Aditya: So, Radesh, I think the generic model will not have access to the hospital specific, you know. Let's say policies, or, let's say, some documents, admission, process, and all those things.

449
00:47:00.120 --> 00:47:00.540
Ritesh Bhagwat: That's right.

450
00:47:00.540 --> 00:47:04.180
Aditya: So with additional documents. We can, you know, add this knowledge to the.

451
00:47:04.450 --> 00:47:28.819
Ritesh Bhagwat: Exactly. Exactly. Yeah. Yeah. And you know, you would just think of it as every country has some specific regularities. Right? The hospital, I mean the government. They regulate something. There's something like that within us. In India. Also, we have some things right, some specific things to specific geographies in India, even State State have their own regulations that this has to be done. So all those things

452
00:47:28.820 --> 00:47:53.689
Ritesh Bhagwat: might. Let's say we want to make specific to a geography. Right. So the all the rules and legalities of hospital running, administration, etcetera, etcetera, have to be loaded into this, and may made it customized to that particular hospital that let's say that hospital is located in Pune. Right? So it has to follow the whatever the municipal Corporation of Pune dictates. Then, after that, whatever the multiple. I mean, what is, whatever

453
00:47:53.690 --> 00:48:01.840
Ritesh Bhagwat: the policies of Maharash to dictate over that, whatever the policies of India dictate. So we are, we have to give all these things, right legalities and.

454
00:48:01.840 --> 00:48:27.979
Ritesh Bhagwat: doctor, and whatever you know, whatever comes, all the peripherals of data specific to India and specific to Mahashtra and specific to Pune. So it becomes that hospital. Now, this might not be available at a very high level, right? Those high, level things. It's very generic thing. So in us. The hospital healthcare works in a different way. In India it works in a different way. Right in London it works in Uk. It works in a different way. So we have to give all these things and make it specific.

455
00:48:28.020 --> 00:48:52.540
Ritesh Bhagwat: So 1st of all, we are making it domain specific. That is, you know, it can answer here it can answer on science tech, healthcare finance. We don't want all this. We are making it specific to healthcare. And then inside this also, wherever the hospital is located, we have to ping those regularities. And then we can deploy this model in our area. So what we are doing is we are taking. These are known as foundational models. By the way.

456
00:48:52.920 --> 00:48:56.410
Ritesh Bhagwat: so a jargon alert. These are known as foundational models.

457
00:48:59.010 --> 00:49:04.840
Ritesh Bhagwat: foundational model. This is a foundational model. You take a foundational model, you tweak it.

458
00:49:05.030 --> 00:49:07.870
Ritesh Bhagwat: It's tweaking is the word which we call it as fine tuning.

459
00:49:08.220 --> 00:49:13.070
Ritesh Bhagwat: We fine tune it, and then we make it for our specific cases.

460
00:49:13.270 --> 00:49:15.060
Ritesh Bhagwat: Okay, so right now.

461
00:49:15.940 --> 00:49:36.849
Ritesh Bhagwat: almost everyone. And in India also, the opportunity lies in doing this. Only I mean, it's very it is almost. I can just say that it is almost impossible that you're going to build an Llm. From scratch. Nobody's going to do that, even. You know the all the tech gurus in India, the real real ones. Okay, not the influencer type stuff. What they are saying is

462
00:49:37.280 --> 00:49:39.989
Ritesh Bhagwat: that the opportunity for India lies in application.

463
00:49:40.320 --> 00:49:59.139
Ritesh Bhagwat: Okay, the things are already built. We have to go ahead and make solutions based on these foundational models. So take a foundational model and you tweak it according to your business requirement. And that is what fine tuning is all about, which is very similar to transfer learning which we have already talked, and I've shared a session. Also.

464
00:50:00.690 --> 00:50:03.989
Ritesh Bhagwat: you understand this specific things about fine tuning.

465
00:50:04.100 --> 00:50:06.220
Ritesh Bhagwat: What this fine tuning actually is.

466
00:50:11.750 --> 00:50:11.990
Shrinidhi: Yeah.

467
00:50:11.990 --> 00:50:14.349
Ritesh Bhagwat: Yes, yes, yeah. Any questions on this.

468
00:50:14.510 --> 00:50:18.620
Ritesh Bhagwat: how we are doing it all the 5 6 sessions remaining. We just do fine tuning on.

469
00:50:21.060 --> 00:50:32.560
Ritesh Bhagwat: Okay. So now. But right? But so we are going to solve next week a number of problems with Bert. So all the classical problems of classification are actually. Now, just give it to Bert.

470
00:50:33.230 --> 00:50:59.159
Ritesh Bhagwat: So machine learning for that classification, you can still do it. But when it comes to language, right when it is a when the data is in a row column format right in the structured data thing, you should always go with the traditional machine learning. It is still the bread and butter everything. Okay, it's not that traditional machine learning is dead. Whenever you are getting data in this format, you should always go with traditional machine learning methods. But the moment you get unstructured data, like images or languages and all

471
00:50:59.160 --> 00:51:05.039
Ritesh Bhagwat: you go to this transformer. That's a neural network based model. So what we will do next week is we'll take this bird.

472
00:51:05.040 --> 00:51:21.509
Ritesh Bhagwat: Okay, we'll go a little bit into architecture. We'll not go. I'll share the research paper with you. But yeah, we'll still go with a little bit, you know, high level architecture, and we'll solve 2, 3 problems at least with this bird. And with this we will discuss the Q. Aq.

473
00:51:22.300 --> 00:51:25.480
Ritesh Bhagwat: okay, and we'll have a solid understanding of that as well.

474
00:51:27.540 --> 00:51:32.109
Ritesh Bhagwat: Okay, so can I. Say, we have got a decent understanding of fine tuning.

475
00:51:34.930 --> 00:51:35.930
Shrinidhi: Yes.

476
00:51:35.930 --> 00:51:47.079
Ritesh Bhagwat: Yeah. And can I say, we have 2. We have a transformer which is encoder, decoder, transformer. We have a transformer which is encoder, only transformer. I mean, we can have, and we can have a transformer which is a decoder, only transformer.

477
00:51:48.200 --> 00:51:49.730
Ritesh Bhagwat: Right? This is also understood.

478
00:51:51.650 --> 00:51:52.350
Shrinidhi: Yep.

479
00:51:52.660 --> 00:51:53.280
Aditya: It is okay.

480
00:51:53.280 --> 00:52:10.650
Ritesh Bhagwat: So yeah, so our next 6 sessions will just be of all these things. Okay, we'll do it. So. And I'll also introduce Claude, Claude, and Chat Gp. To you. So we'll take a break, and I'll tell you something how to get started with Chat Gpt, and we can call it a day today, so next week will be whole hands on.

481
00:52:10.830 --> 00:52:15.210
Ritesh Bhagwat: But I hope these things are clear. If it is not clear you please ask me again.

482
00:52:15.550 --> 00:52:17.310
Ritesh Bhagwat: Okay, anything.

483
00:52:17.310 --> 00:52:24.109
Aditya: Sure. Just one thing like the fine tuning is kind of a subset of transfer learning, or is it the other way around.

484
00:52:24.470 --> 00:52:26.850
Ritesh Bhagwat: No, it is called a subset of transfer learning.

485
00:52:27.210 --> 00:52:37.060
Ritesh Bhagwat: Okay to me to me. Actually, let me be very frank with you. To me. Okay, I I mean, people may not agree with this, but I think this is, you know, they have just

486
00:52:37.540 --> 00:52:42.330
Ritesh Bhagwat: rebranded it. There was something that stands for learning. You know how it industry works right.

487
00:52:43.270 --> 00:53:04.449
Ritesh Bhagwat: branded it as fine tuning that it is just transfer learning for me, but they call it, you know if you, if you go ahead and ask questions on Google or Chatgpt, what is the difference between transfer learning and fine tuning, it will tell you some subtle differences that this is a higher umbrella and fine tuning comes under it. But me, it is just rebranding. Okay, I might be wrong.

488
00:53:04.761 --> 00:53:19.330
Ritesh Bhagwat: I mean. To my mind it is always the same thing. But you know how it works. Right? Something, some when you move the yeah, the old. What that is saying, like old wine, new bottle, whatever that is. That is this this thing. It is just transfer learning.

489
00:53:19.635 --> 00:53:25.480
Ritesh Bhagwat: making it in transfer learning. Also, we use existing, we tweak it into fine tuning. Also, we do and tweak. Okay. Now.

490
00:53:25.816 --> 00:53:36.239
Aditya: In fine tuning. We are kind of updating the weights of all the weights of the pre trained model. Right in transfer learning. We are adding layers, and then, or maybe.

491
00:53:36.240 --> 00:53:37.809
Ritesh Bhagwat: Fine tuning. Also we can do that.

492
00:53:37.810 --> 00:53:39.380
Aditya: Okay. Okay. Okay.

493
00:53:39.600 --> 00:53:40.980
Ritesh Bhagwat: Okay, we can do that. Whatever.

494
00:53:40.980 --> 00:53:42.250
Aditya: Anonymous. Yeah.

495
00:53:42.250 --> 00:53:51.900
Ritesh Bhagwat: Yeah, yeah, so it is just, you know, just it is just, you know, if you're solving a computer imaging problem, there is a way to solve it. If you're solving an Nlp problem, there is a way to solve

496
00:53:52.150 --> 00:53:52.920
Ritesh Bhagwat: right.

497
00:53:54.250 --> 00:53:54.990
Aditya: Right, right.

498
00:53:54.990 --> 00:54:04.940
Ritesh Bhagwat: Right. So that depends on the type of this. This thing you're solving now. Tomorrow what they might come up with they might come up with a little bit of tweak of something, and they might call it something else.

499
00:54:05.270 --> 00:54:13.320
Ritesh Bhagwat: and that will be a brand, and then the courses will build will be built around that. You learn this like prompt engineering. Right? They have built prompt engineering as a separate thing.

500
00:54:13.400 --> 00:54:26.600
Ritesh Bhagwat: And they're, you know, giving certified courses of broad engineering. Also, it is just a marketing gimmick. If something new will come up, and then they will form an industry around it, then it will slowly fade, fade away after a few months.

501
00:54:26.610 --> 00:54:30.230
Ritesh Bhagwat: If you have been, you know, following this field, you will see that

502
00:54:30.781 --> 00:54:37.059
Ritesh Bhagwat: that things come things pop up suddenly, they become the most important. You you will, cannot survive without it, and then it goes down.

503
00:54:37.350 --> 00:55:00.050
Ritesh Bhagwat: So it's just a marketing exercise, right? So, but still there's a difference. There is subtle difference. You can go ahead, go and see on Chat Gpt. But if you look at if you ask Chat gpt, but you know, if you ask it, a question difference between this and this, and then it then you ask that chat. But isn't it the same. So it will tell you, yeah, it's technically the same. But this, this, this like, like that type of answer you will get.

504
00:55:00.610 --> 00:55:03.070
Ritesh Bhagwat: yeah, yeah, that's got it. Yeah, yeah, yeah.

505
00:55:04.640 --> 00:55:09.239
Ritesh Bhagwat: Okay. So let's take a 8 min break, and then we'll get introduced to open air.

506
00:55:15.614 --> 00:55:19.006
Shubham Gupta: Hi, ritesh! I actually joined a little late

507
00:55:19.430 --> 00:55:19.870
Ritesh Bhagwat: Yes.

508
00:55:19.870 --> 00:55:25.860
Shubham Gupta: I wanted to confirm that we were discussing that key value. Query last time.

509
00:55:25.860 --> 00:55:27.259
Ritesh Bhagwat: Yes, yes, yes, yes.

510
00:55:28.860 --> 00:55:29.810
Shubham Gupta: Yeah, yeah.

511
00:55:33.970 --> 00:56:01.109
Ritesh Bhagwat: Correct. Correct? Yeah. So at the start of the session, what I said is, because what was happening was, we were not able to discuss this fine tuning thing right? And what is fine tuning? We are getting stuck in a loop for last 2 sessions in that transformer architecture. Difference? Only right? So what I told at the start of the session was, we are still pending on key query, key values. What we are going to do is next week, when we are. So we are going to solve the bird problem. What we'll do. We'll talk about that and then implement it straight away.

512
00:56:01.140 --> 00:56:17.889
Ritesh Bhagwat: so that the the the gap that is coming in the lack of understanding that will go away. But before that, because with bird we have to do all the understand fine tuning and all that stuff. So we will take that today and then come to key queries and values next week, along with the practical application.

513
00:56:19.770 --> 00:56:36.700
Ritesh Bhagwat: Okay, yeah. So that's why you might have missed that. Okay. So you might have thought that what's going around. We have just started this. Okay, we'll come to that. Don't worry. Okay. Meantime, take this time to revise more about that key queries and values, and we will come and talk about it next week is full key query, metrics, and but that's it.

514
00:56:37.210 --> 00:56:38.019
Shubham Gupta: Got it.

515
00:56:38.220 --> 00:56:38.780
Ritesh Bhagwat: Okay.

516
01:04:58.200 --> 01:05:00.110
Ritesh Bhagwat: okay. So Shubham is back

517
01:05:00.985 --> 01:05:07.174
Ritesh Bhagwat: so Shubam, you you missed the initial part. But were you at that time of that encoder decoder

518
01:05:07.780 --> 01:05:08.720
Ritesh Bhagwat: discussion.

519
01:05:09.390 --> 01:05:12.860
Shubham Gupta: Yes, yes, I joined like only 2, 3 min late.

520
01:05:12.860 --> 01:05:18.230
Ritesh Bhagwat: Okay, okay, so you were part of that vegetable making and all that analogy I gave.

521
01:05:18.520 --> 01:05:19.480
Shubham Gupta: Yes, yes, yes.

522
01:05:19.480 --> 01:05:21.050
Ritesh Bhagwat: Okay, okay, that time is fine.

523
01:05:27.260 --> 01:05:29.720
Ritesh Bhagwat: It will. We are waiting for others.

524
01:05:29.880 --> 01:05:37.630
Ritesh Bhagwat: We need these here. We'll just, you know, finish it off in next 1015 min. Just wanted to introduce Chat Gpt. Api to you.

525
01:05:38.142 --> 01:05:40.769
Ritesh Bhagwat: So that, you know we get the ball rolling.

526
01:05:51.080 --> 01:05:53.809
Ritesh Bhagwat: So, Nishant, if I actually need the update.

527
01:05:53.950 --> 01:05:55.499
Ritesh Bhagwat: We are waiting for Aditya.

528
01:06:21.800 --> 01:06:23.810
Ritesh Bhagwat: So any questions till now.

529
01:06:24.630 --> 01:06:26.360
Ritesh Bhagwat: Srinith, you are still here. Right?

530
01:06:26.870 --> 01:06:27.630
Ritesh Bhagwat: Yeah, we can.

531
01:06:29.160 --> 01:06:40.480
Ritesh Bhagwat: Yeah. By the way, you know we are going over time over time in the sense that there were 24 sessions. We are going to go over time, so Nishant knows it. Nishant has taken a couple of my earlier sessions also.

532
01:06:40.630 --> 01:06:51.020
Ritesh Bhagwat: So I hope you're okay with it. Right? I think you should be okay. But because, you know we are not like coaching class and trying to finish. We are taking over time, but I will try and wrap it up by December end.

533
01:06:51.240 --> 01:06:58.899
Ritesh Bhagwat: Okay, everything. In next 4 or 5 sessions we'll we'll do a lot of hands on so. But still, I I hope you all are okay with it. Right.

534
01:07:00.510 --> 01:07:01.189
Shubham Gupta: Yes, we do.

535
01:07:01.190 --> 01:07:02.880
Ritesh Bhagwat: It's just yeah, yeah.

536
01:07:03.130 --> 01:07:07.080
Shrinidhi: Everything while others. So I just want to share some

537
01:07:07.220 --> 01:07:13.209
Shrinidhi: thing like this week I had to conduct one boot camp in my office and one of the topics was embeddings. So

538
01:07:13.210 --> 01:07:16.080
Shrinidhi: okay, okay, had to give a 45 min talk on that

539
01:07:16.230 --> 01:07:20.580
Shrinidhi: and use all the knowledge information that we learned over last April.

540
01:07:20.580 --> 01:07:20.990
Ritesh Bhagwat: Great.

541
01:07:20.990 --> 01:07:23.580
Shrinidhi: And it was very well appreciated. Everyone and.

542
01:07:23.580 --> 01:07:25.329
Ritesh Bhagwat: Whatever. Great. Great. Yeah. Happy.

543
01:07:25.330 --> 01:07:28.030
Shrinidhi: You do that because of the sessions that you did so. I thought.

544
01:07:28.030 --> 01:07:30.960
Ritesh Bhagwat: Yeah, I'm yeah. I'm so happy for that. Yeah.

545
01:07:31.590 --> 01:07:50.920
Ritesh Bhagwat: this is actually me. This, all things make make me very, very happy. I take a lot of pride in the sense that you know, because I have not kept this as my primary source of income. So you know, I I teach slowly. I take my time. I try not to do this as a coaching class, and when you say some things like this I I feel very happy it is all.

546
01:07:50.940 --> 01:07:53.359
Ritesh Bhagwat: No, it it feels very nice to me.

547
01:07:53.990 --> 01:07:55.160
Ritesh Bhagwat: Thank you. Thank you. Thank you.

548
01:07:55.160 --> 01:07:56.709
Ritesh Bhagwat: Yeah. Thanks. Thanks to you also.

549
01:07:59.280 --> 01:08:01.669
Ritesh Bhagwat: Yeah. So Ali Ali carry you back

550
01:08:03.330 --> 01:08:16.670
Ritesh Bhagwat: is not here. Okay, we'll start. We will just go ahead and quickly. You know, I'm just going to show you maybe 10 min, everything that we did with Gemini, we are actually going to do with chat Gpt. Okay.

551
01:08:16.700 --> 01:08:23.569
Ritesh Bhagwat: So for Chat Gpt, what you need to do is these, basically, you have to install.

552
01:08:24.011 --> 01:08:50.279
Ritesh Bhagwat: This is all part python, dot. Nvc, one skyscape and you can just write the requirement file. You have to install. Pip, install open. AI, okay, like in Gemini, we had to install the Gemini. We have to install this open. AI. Okay, now, what is happening is I figured it out that after installing it sometimes it is not running. So what? The what I'm going to share this file with you just run this for the safety pip, install upgrade. Open AI white queue is just quite

553
01:08:50.359 --> 01:09:04.659
Ritesh Bhagwat: okay. So you go ahead and you update this. This is done. It should work. Okay. And now, I'm just telling you and just get just the getting started type of thing. Okay, this, we are not trying to solve any problem. We'll solve these problems later. So

554
01:09:05.500 --> 01:09:12.179
Ritesh Bhagwat: you run this. I've already run this. Now, import OS, dot env load dot Nv. And

555
01:09:12.220 --> 01:09:18.650
Ritesh Bhagwat: instead of Gemini we are loading openai. So if you don't remember what this dot envy and all this stuff is, you just go ahead

556
01:09:18.689 --> 01:09:35.540
Ritesh Bhagwat: and you just take our Gemini Gemini sessions, you will, you will. You know. Remember what all this this will load the local environment. So I have kept my, if you see this dot inv variable here I have kept my Api key here in in for for open and named it as Open Api Key.

557
01:09:35.729 --> 01:09:38.330
Ritesh Bhagwat: So I run it. Now you can go here.

558
01:09:38.340 --> 01:09:42.839
Ritesh Bhagwat: platform dot this thing, and from here you can create an open, a id.

559
01:09:42.920 --> 01:09:46.809
Ritesh Bhagwat: Okay, you just log in log in with your credentials here, and it will give you

560
01:09:47.100 --> 01:09:49.870
Ritesh Bhagwat: it, will. It will give you access to your Openai.

561
01:09:50.170 --> 01:09:57.780
Ritesh Bhagwat: Right? So I I'm sharing this file. You can just go here. Okay, so this I have imported twice. This is not needed. I've loaded the dot. Inv

562
01:09:57.930 --> 01:10:09.030
Ritesh Bhagwat: Openai Key is equal to OS dot get environment, the Openai Key name. This is exactly like Gemini. I hope you remember this, but things that we did with our Gemini exactly the same way, but with open.

563
01:10:11.800 --> 01:10:16.230
Ritesh Bhagwat: Anyone who remembers this, or What is this? OS dot getty in me or not?

564
01:10:16.820 --> 01:10:17.619
Aditya: Yes, it is.

565
01:10:17.620 --> 01:10:21.689
Ritesh Bhagwat: Yeah, you remember? Yeah, you you have actually added to build a streamnet application. So you should.

566
01:10:22.070 --> 01:10:22.779
Aditya: Yeah, yeah.

567
01:10:22.780 --> 01:10:35.860
Ritesh Bhagwat: I know you worry about. Yeah. So we are just doing this with open instead of using Gemini, you use the Openai. That's it. Okay, you do this. And just for getting started, I'm just going to run this

568
01:10:36.450 --> 01:10:40.800
Ritesh Bhagwat: from Openai Import Openai. We are making a client

569
01:10:41.240 --> 01:10:47.650
Ritesh Bhagwat: for client and open client or ABC, you can just write. ABC, now, client is we are calling it on open air like this.

570
01:10:47.860 --> 01:10:50.630
Ritesh Bhagwat: and we are just seeing what are the models available.

571
01:10:50.710 --> 01:10:55.309
Ritesh Bhagwat: So client, dot model dot list. You can just see the list of client.

572
01:10:55.440 --> 01:10:56.460
Ritesh Bhagwat: Now it is.

573
01:10:56.660 --> 01:11:07.099
Ritesh Bhagwat: It is giving this in such a weird manner. Right? It is giving this so better is right for model in clients. Dot model dot print print model, so it will print the model one time.

574
01:11:08.020 --> 01:11:09.770
Ritesh Bhagwat: These are all the models available.

575
01:11:12.010 --> 01:11:14.969
Ritesh Bhagwat: So all, or one preview, or one Mini

576
01:11:15.280 --> 01:11:20.799
Ritesh Bhagwat: rally one, you might have a Gpt. 3.5 Turbo Gpt. 3.5. There's baggage.

577
01:11:21.490 --> 01:11:26.570
Ritesh Bhagwat: Darwin, C. And all these models available. Now, what you can do whisper. By the way, this is used a lot.

578
01:11:26.630 --> 01:11:28.830
Ritesh Bhagwat: Some open source versions are there in

579
01:11:29.508 --> 01:11:44.619
Ritesh Bhagwat: hugging face. Also, we'll use use those. Okay. What you can do is we'll do this in any case, we'll do this. But you go ahead and you start the exactly the way we did things with gemini. You can do the same with open air. So

580
01:11:45.279 --> 01:12:05.670
Ritesh Bhagwat: so what I would urge everybody. Is Aditya. What you can do is what we build it streamlit, right? You use gemini there for the for the purpose of solving the problem right? So use open AI models for solving the problem, and the streamline code will remain the same. So what is going to happen is you're using open. That's that is the only difference.

581
01:12:06.300 --> 01:12:11.260
Aditya: Yes, it is but the the code, for you know that maintaining history and all that will change.

582
01:12:11.260 --> 01:12:11.580
Ritesh Bhagwat: Yeah.

583
01:12:11.580 --> 01:12:12.589
Aditya: With the with the open air.

584
01:12:12.590 --> 01:12:15.140
Ritesh Bhagwat: Yes, yes, that's correct. That's yeah.

585
01:12:15.140 --> 01:12:15.480
Aditya: Okay.

586
01:12:15.480 --> 01:12:39.539
Ritesh Bhagwat: You. You started as a standalone thing. Do some R&D, we are going to go ahead and explore these models. I'll tell you, which are more important, and which are not important. But you know, because we have done all these with Gemini, you yourself on your own, can do with all this, if you, if you know one, you know everything. It is like that. But still we I'll I'll tell you what are important, what is not important. I recently got to work on open thankfully. So I have a you know.

587
01:12:39.630 --> 01:12:49.120
Ritesh Bhagwat: I mean, I have worked a lot. So I have some knowledge of working on open. So I'll I'll tell you this, if you could see text embedding large and all, you can just go ahead and start using this.

588
01:12:49.180 --> 01:12:59.008
Ritesh Bhagwat: So go ahead. Start exploring, what we will do next week is Bert Aqv. And we'll solve one problem with, at least one problem with this

589
01:12:59.380 --> 01:13:00.430
Ritesh Bhagwat: open a it.

590
01:13:01.350 --> 01:13:05.229
Ritesh Bhagwat: Okay? So we have last session. This session.

591
01:13:05.360 --> 01:13:25.650
Ritesh Bhagwat: Both of this are the foundation for all next 5 6 sessions that you're going. We are going to study. So you know, take your time to again revise the Kqw Kqv. Metrics type and we will. We will just conclude it in the 1st part of the session next. Okay? Again, I'm giving you one more. You know this thing to learn. And if somebody wants to present, I'm not going to force anybody.

592
01:13:25.650 --> 01:13:35.809
Ritesh Bhagwat: Learn this. If you get time somebody wants to present on Kqv. He can present. If not, then I'll take it up, we will conclude it, and we will implement it in bot.

593
01:13:38.310 --> 01:13:43.269
Ritesh Bhagwat: Okay, so that is it from my side? Any questions. Otherwise I'm done.

594
01:13:47.840 --> 01:13:51.750
Ritesh Bhagwat: Yeah, no question. So I'm quite happy, Siniti. You told me that you presented this

595
01:13:52.102 --> 01:14:04.379
Ritesh Bhagwat: session so congratulations to you, and let's meet on next Sunday. And you know we'll have some. Okay, come with the mindset of next Sunday that it is a problem. It is going to be a problem solving session, not a political session.

596
01:14:06.700 --> 01:14:07.760
Shrinidhi: Service, Thanks.

597
01:14:07.760 --> 01:14:13.439
Ritesh Bhagwat: Yeah, yeah, thanks. Thanks. Everyone. Have a good week. And yeah, let's be talking. Then enjoy the match. The cricket, I mean, if

598
01:14:13.580 --> 01:14:17.182
Ritesh Bhagwat: I hope somebody will be degraded. They are going doing very good, actually.

599
01:14:17.460 --> 01:14:19.510
Akshay: Yeah, we'll be winning this match.

600
01:14:19.650 --> 01:14:22.489
Ritesh Bhagwat: Yeah, yeah, it seems. So. Yeah, 2, 75 for one.

601
01:14:23.010 --> 01:14:23.800
Akshay: Yes.

602
01:14:23.800 --> 01:14:24.829
Ritesh Bhagwat: It'd be great. Yeah.

603
01:14:25.110 --> 01:14:27.160
Akshay: It's everything is one sided in my hospital.

604
01:14:27.800 --> 01:14:28.560
Ritesh Bhagwat: Yes, all.

605
01:14:30.210 --> 01:14:33.449
Ritesh Bhagwat: Yes, yes, that's true. Yesterday, also, once I did today, also.

606
01:14:34.980 --> 01:14:37.720
Ritesh Bhagwat: Okay, bye-bye. Have a good week. Let's meet on next time.

607
01:14:38.490 --> 01:14:38.969
Shrinidhi: Thank you.

608
01:14:38.970 --> 01:14:40.379
Aditya: Okay, thank you. Ritesh. Bye, bye.

609
01:14:40.380 --> 01:14:41.379
Ritesh Bhagwat: Yeah. Thanks. Bye.

